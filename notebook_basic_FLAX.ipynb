{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple MLP with FLAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install flax if haven't done so\n",
    "!pip install -q flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a toy example using flax\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "import optax # optimization library for JAX from deepmind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "[0 0] [4146024105  967050713] [2718843009 1272950319] \n",
      "\n",
      "x: [-2.6105583   0.03385283  1.0863333  -1.4802988   0.48895672  1.062516\n",
      "  0.54174834  0.0170228   0.2722685   0.30522448]\n",
      "\n",
      "params: frozen_dict_keys(['params'])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        bias: (5,),\n",
       "        kernel: (10, 5),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dense layer\n",
    "model = nn.Dense(features=5)\n",
    "\n",
    "# Psedo random number generation\n",
    "key = jax.random.PRNGKey(0)\n",
    "print(key)\n",
    "key1, key2 = jax.random.split(key)\n",
    "print(key, key1, key2, '\\n')\n",
    "\n",
    "# initialize input\n",
    "x = jax.random.normal(key1, (10,))\n",
    "params = model.init(key2, x)\n",
    "print(\"x: {}\\n\\nparams: {}\\n\".format(x, params.keys()))\n",
    "\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3721193   0.61131495  0.6442836   2.2192965  -1.1271116 ]\n"
     ]
    }
   ],
   "source": [
    "y = model.apply(params, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a MLP module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Define a MLP with FLAX's pre-defined layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "from flax.core import freeze, unfreeze\n",
    "from typing import Sequence\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [nn.Dense(feat) for feat in self.features]\n",
    "        # self.layer1 = nn.Dense(feat1)\n",
    "        self.num_layers = len(self.layers)\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i != self.num_layers-1:\n",
    "                x = nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized param shapes:\n",
      " {'params': {'layers_0': {'bias': (2,), 'kernel': (4, 2)}, 'layers_1': {'bias': (3,), 'kernel': (2, 3)}, 'layers_2': {'bias': (4,), 'kernel': (3, 4)}, 'layers_3': {'bias': (5,), 'kernel': (4, 5)}}}\n",
      "output:\n",
      " [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import jax.random as random\n",
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = random.uniform(key1, (4,4))\n",
    "\n",
    "model = MLP(features=[2,3,4,5])\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized param shapes:\\n', \n",
    "      jax.tree_util.tree_map(jnp.shape, unfreeze(params)))\n",
    "\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Customize layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from jax import lax # lib for primitive ops\n",
    "\n",
    "class CustomDense(nn.Module):\n",
    "    features: int\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros\n",
    "\n",
    "    @nn.compact # about this decorator: https://github.com/google-research/vision_transformer/issues/118\n",
    "    def __call__(self, inputs):\n",
    "        kernel = self.param('kernel',\n",
    "                            self.kernel_init,\n",
    "                            (inputs.shape[-1], self.features)\n",
    "        )\n",
    "        y = lax.dot_general(inputs, kernel,\n",
    "                        (((inputs.ndim - 1,), (0,)), ((), ())),)\n",
    "        bias = self.param('bias', self.bias_init, (self.features,))\n",
    "        y = y + bias\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameters:\n",
      " FrozenDict({\n",
      "    params: {\n",
      "        kernel: DeviceArray([[ 0.61506   , -0.22728713,  0.6054702 ],\n",
      "                     [-0.29617992,  1.1232013 , -0.879759  ],\n",
      "                     [-0.35162622,  0.3806491 ,  0.6893246 ],\n",
      "                     [-0.1151355 ,  0.04567898, -1.091212  ]], dtype=float32),\n",
      "        bias: DeviceArray([0., 0., 0.], dtype=float32),\n",
      "    },\n",
      "})\n",
      "output:\n",
      " [[-0.02996203  1.102088   -0.6660265 ]\n",
      " [-0.31092793  0.63239413 -0.53678817]\n",
      " [ 0.01424009  0.9424717  -0.63561463]\n",
      " [ 0.3681896   0.3586519  -0.00459218]]\n"
     ]
    }
   ],
   "source": [
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = random.uniform(key1, (4,4))\n",
    "\n",
    "model = CustomDense(features=3)\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameters:\\n', params)\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Gradient descent without FLAX (pure JAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    params: {\n",
      "        bias: (5,),\n",
      "        kernel: (10, 5),\n",
      "    },\n",
      "})\n",
      "x_shape: (20, 10), y_shape: (20, 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent for linear regression\n",
    "n_samples = 20\n",
    "x_dim, y_dim = 10, 5\n",
    "\n",
    "# create parameters to optimize\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x = random.normal(key1, (10,)) # Dummy input\n",
    "\n",
    "model = nn.Dense(features=5)\n",
    "params = model.init(key2, x) # Initialization call\n",
    "print(jax.tree_util.tree_map(lambda x: x.shape, params)) # Checking output shapes\n",
    "\n",
    "# create optimized parameters \n",
    "key = random.PRNGKey(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim,))\n",
    "true_params = freeze({\n",
    "    'params':{\n",
    "        'bias': b,\n",
    "        'kernel': W\n",
    "    }\n",
    "})\n",
    "\n",
    "# generate GT data for training\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "rand_noise = 0.1*random.normal(key_noise, (n_samples, y_dim))\n",
    "y_samples = jnp.dot(x_samples, W) + b + rand_noise\n",
    "\n",
    "print('x_shape: {}, y_shape: {}\\n'.format(x_samples.shape, y_samples.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss with true W,b: 0.023639793\n"
     ]
    }
   ],
   "source": [
    "# function to get loss\n",
    "@jax.jit\n",
    "def mse(params, x_batch, y_batch):\n",
    "    def sqrt_error(x, y):\n",
    "        pred = model.apply(params, x)\n",
    "        return jnp.inner(y-pred, y-pred)/2.0\n",
    "    # clever use of vmap, jax's cool feature\n",
    "    return jnp.mean(jax.vmap(sqrt_error)(x_batch, y_batch), axis=0)\n",
    "\n",
    "# function to get gradient\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "# function to update params\n",
    "@jax.jit\n",
    "def update_params(params, lr, grads):\n",
    "    params = jax.tree_util.tree_map(\n",
    "        lambda p, g: p - lr*g, params, grads\n",
    "    )\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss with true W,b: 0.023639793\n",
      "loss step 0: 0.011568372137844563\n",
      "loss step 10: 0.011568372137844563\n",
      "loss step 20: 0.011568372137844563\n",
      "loss step 30: 0.011568372137844563\n"
     ]
    }
   ],
   "source": [
    "# test getting loss\n",
    "lr = 0.3\n",
    "loss = mse(true_params, x_samples, y_samples)\n",
    "print('loss with true W,b:', loss)\n",
    "\n",
    "# test getting gradients\n",
    "loss, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "#print('loss:{}\\n gradient: {}\\n'.format(loss, grads))\n",
    "\n",
    "# test updating params\n",
    "#print('original params: {}'.format(params))\n",
    "params = update_params(params, lr, grads)\n",
    "#print('updated params: {}'.format(params))\n",
    "\n",
    "# perform back-propagation for multiple iterations\n",
    "for i in range(0, 31):\n",
    "    loss, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    params = update_params(params, lr, grads)\n",
    "    if i % 10 == 0:\n",
    "        print('loss step {}: {}'.format(i, loss))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Gradient Descent with *Optax* (lib from deepmind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 0: 0.011568371206521988\n",
      "loss at step 10: 0.011568371206521988\n",
      "loss at step 20: 0.011568371206521988\n",
      "loss at step 30: 0.011568371206521988\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "optimizer = optax.sgd(learning_rate=lr)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "for i in range(31):\n",
    "    loss, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print('loss at step {}: {}'.format(i, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize params for export and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        bias: DeviceArray([-1.4540124 , -2.0262275 ,  2.0806599 ,  1.2201837 ,\n",
       "                     -0.99645793], dtype=float32),\n",
       "        kernel: DeviceArray([[ 1.0106655 ,  0.19014445,  0.04533757, -0.9272265 ,\n",
       "                       0.3472048 ],\n",
       "                     [ 1.732027  ,  0.99013054,  1.1662259 ,  1.102798  ,\n",
       "                      -0.10575476],\n",
       "                     [-1.2009128 ,  0.28837118,  1.4176372 ,  0.12073042,\n",
       "                      -1.3132594 ],\n",
       "                     [-1.194495  , -0.18993127,  0.03379178,  1.3165966 ,\n",
       "                       0.07995866],\n",
       "                     [ 0.14103451,  1.3738064 , -1.3162082 ,  0.5340303 ,\n",
       "                      -2.2396488 ],\n",
       "                     [ 0.5643062 ,  0.8136104 ,  0.31888482,  0.53592736,\n",
       "                       0.903514  ],\n",
       "                     [-0.3794808 ,  1.7408438 ,  1.0788052 , -0.5041857 ,\n",
       "                       0.9286824 ],\n",
       "                     [ 0.97013855, -1.3158665 ,  0.33630857,  0.8094122 ,\n",
       "                      -1.2024579 ],\n",
       "                     [ 1.0198247 , -0.619828  ,  1.0822717 , -1.8385581 ,\n",
       "                      -0.4579065 ],\n",
       "                     [-0.64384246,  0.4564922 , -1.1331041 , -0.6855652 ,\n",
       "                       0.17010559]], dtype=float32),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# serialize params for export\n",
    "from flax import serialization\n",
    "bytes_output = serialization.to_bytes(params)   #choice1\n",
    "dict_output = serialization.to_state_dict(params) #choice2\n",
    "\n",
    "# import params\n",
    "#serialization.from_bytes(params, bytes_output)  # choice1\n",
    "serialization.from_state_dict(params, dict_output)   # choice2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73f72bc72549e98a84f6f70716c3cfd3763eb776afa04f85c632de90851415ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

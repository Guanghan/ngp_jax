{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opt import get_opts\n",
    "from config import Config\n",
    "from models import BasicNeRF\n",
    "from model_utils import generate_rays, perform_volume_rendering\n",
    "from render import pose_spherical\n",
    "\n",
    "import gin\n",
    "from pickletools import optimize\n",
    "\n",
    "import numpy as np\n",
    "from jax import numpy as jnp\n",
    "from jax import jit, pmap, value_and_grad, lax, \\\n",
    "                local_device_count, random, \\\n",
    "                device_put_replicated, local_devices\n",
    "\n",
    "from flax.jax_utils import unreplicate\n",
    "from flax.training import train_state, common_utils\n",
    "import optax\n",
    "\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load configurations and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_opts()\n",
    "gin.parse_config_file(args.gin_config_path)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# load data\n",
    "data = np.load(\"tiny_nerf_data.npz\")\n",
    "images = data[\"images\"]\n",
    "poses = data[\"poses\"]\n",
    "focal = float(data[\"focal\"])\n",
    "\n",
    "# split data into train and val\n",
    "_, img_ht, img_wid, _ = images.shape\n",
    "train_images, train_poses = images[:100], poses[:100]\n",
    "val_image, val_pose = images[101], poses[101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(key, input_pts_shape):\n",
    "    # create model\n",
    "    model = BasicNeRF()\n",
    "\n",
    "    # init model\n",
    "    init_params = jit(model.init)({\"params\":key}, jnp.ones(input_pts_shape))\n",
    "\n",
    "    return model, init_params[\"params\"]\n",
    "\n",
    "\n",
    "def train_step(state, batch, rng):\n",
    "    \"\"\"\n",
    "    Training Step:\n",
    "    state is comb of model state and optimizer state\n",
    "    \"\"\"\n",
    "    inputs, targets = batch\n",
    "\n",
    "    # compute loss in a stateless manner to save memory\n",
    "    def loss_fn(params):\n",
    "        ray_origins, ray_directions = inputs\n",
    "        model_fn = lambda x: state.apply_fn({\"params\": params}, x)\n",
    "        pred_rgbs, *_ = perform_volume_rendering(model_fn, ray_origins, ray_directions, rng)\n",
    "        return jnp.mean( (pred_rgbs - targets)**2 )\n",
    "\n",
    "    # get loss value and gradients    \n",
    "    train_loss, gradients = value_and_grad(loss_fn)(state.params)\n",
    "\n",
    "    # get averaged train loss\n",
    "    train_loss = jnp.mean(train_loss)\n",
    "\n",
    "    # compute all-reduce mean on gradients over the pmapped axis\n",
    "    gradients = lax.pmean(gradients, axis_name=\"batch\")\n",
    "\n",
    "    # update model params & optimizer state\n",
    "    new_state = state.apply_gradients(grads=gradients)\n",
    "\n",
    "    # compute PSNR\n",
    "    train_psnr = -10.0 * jnp.log(train_loss) / jnp.log(10.0)\n",
    "    return train_loss, train_psnr, new_state\n",
    "\n",
    "\n",
    "@jit\n",
    "def validation_step(state, val_rays, val_img):\n",
    "    \"\"\"\n",
    "    Validation\n",
    "    \"\"\"\n",
    "    model_fn = lambda x: state.apply_fn({\"params\": state.params}, x)\n",
    "    ray_origins, ray_directions = val_rays\n",
    "\n",
    "    pred_rgb, pred_depth, *_ = perform_volume_rendering(model_fn, ray_origins, ray_directions)\n",
    "\n",
    "    val_loss = jnp.mean( (pred_rgb - val_img)**2 )\n",
    "    val_psnr = -10.0 * jnp.log(val_loss) / jnp.log(10.0)\n",
    "    return pred_rgb, pred_depth, val_psnr, val_loss\n",
    "\n",
    "\n",
    "def train_and_evaluate(state, train_step_fn, validation_step_fn):\n",
    "    train_loss_history, train_psnr_history = [], []\n",
    "    val_loss_history, val_psnr_history = [], []\n",
    "\n",
    "    for epoch in tqdm(range(config.train_epochs)):\n",
    "        # shard random number generators\n",
    "        rng_index, rng_epoch = random.split(random.fold_in(rng, epoch))\n",
    "        sharded_rngs = common_utils.shard_prng_key(rng_epoch)\n",
    "\n",
    "        # create training batch \n",
    "        train_index = random.randint(rng_index, (n_devices,), minval=0, maxval=len(train_rays))\n",
    "        train_batch = train_rays[tuple(train_index), ...],\\\n",
    "                      train_images[tuple(train_index), ...]\n",
    "        \n",
    "        # perform training step\n",
    "        train_loss, train_psnr, state = train_step_fn(state, train_batch, sharded_rngs)\n",
    "        avg_train_loss = np.asarray(np.mean(train_loss))\n",
    "        avg_train_psnr = np.asarray(np.mean(train_psnr))\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        train_psnr_history.append(avg_train_psnr)\n",
    "        \n",
    "        # logging\n",
    "        print(\"Train loss @ epoch {}: {}\".format(epoch, avg_train_loss))\n",
    "        print(\"Train psnr @ epoch {}: {}\".format(epoch, avg_train_psnr))\n",
    "\n",
    "        # perform validation step\n",
    "        validation_state = unreplicate(state)\n",
    "        rgb, depth, val_psnr, val_loss = validation_step_fn(validation_state)\n",
    "        avg_val_loss = np.asarray(np.mean(val_loss))\n",
    "        avg_val_psnr = np.asarray(np.mean(val_psnr))\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        val_psnr_history.append(avg_val_psnr)\n",
    "        \n",
    "        # logging\n",
    "        print(\"Val loss @ epoch {}: {}\".format(epoch, avg_train_loss))\n",
    "        print(\"Val psnr @ epoch {}: {}\".format(epoch, avg_train_psnr))\n",
    "\n",
    "        # plot result at every plot interval\n",
    "        if epoch % config.plot_interval == 0:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\n",
    "            ax1.imshow(rgb)\n",
    "            ax1.set_title(\"Predicted RGB at epoch: {}\".format(epoch))\n",
    "            ax1.axis('off')\n",
    "            ax2.imshow(depth)\n",
    "            ax2.set_title('Predicted RGB at epoch: {}'.format(epoch))\n",
    "            ax2.axis('off')\n",
    "            plt.show()\n",
    "    \n",
    "    inference_state = unreplicate(state)\n",
    "    history = {\n",
    "        'train_loss': train_loss_history,\n",
    "        'train_psnr': train_psnr_history,\n",
    "        'val_loss': val_loss_history,\n",
    "        'val_psnr': val_psnr_history\n",
    "    }\n",
    "    return state, inference_state, history\n",
    "\n",
    "\n",
    "\n",
    "    video_angle = jnp.linspace(0.0, 360.0, 120, endpoint=False)\n",
    "    camera_to_world_transform = map(lambda th: pose_spherical(th, -30.0, 4.0), video_angle)\n",
    "    rays = np.stack(list(map(\n",
    "        lambda x: generate_rays(\n",
    "            img_ht, img_wid, focal, x[:3, :4]\n",
    "        ), camera_to_world_transform\n",
    "    )))\n",
    "    rgb_frames, depth_frames, acc_maps, disparity_maps, opacities = lax.map(get_renderings, rays)\n",
    "    rgb_frames = np.asarray(rgb_frames)\n",
    "    depth_frames = np.asarray(depth_frames)\n",
    "    acc_maps = np.asarray(acc_maps * 255.)\n",
    "    disparity_maps = np.asarray(disparity_maps * 255.)\n",
    "    return rgb_frames, depth_frames, acc_maps, disparity_maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate rays and init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation rays\n",
    "train_rays = np.stack(list(map(lambda x: generate_rays(img_ht, img_wid, focal, x), train_poses)))\n",
    "val_rays = generate_rays(img_ht, img_wid, focal, val_pose)\n",
    "\n",
    "# number of devices\n",
    "n_devices = local_device_count()\n",
    "\n",
    "# rand number generation\n",
    "key, rng = random.split(random.PRNGKey(0))\n",
    "\n",
    "# init the model\n",
    "model, params = init_model(key, (img_ht*img_wid, 3))\n",
    "\n",
    "# optimizer\n",
    "optimizer = optax.adam(learning_rate=config.lr)\n",
    "\n",
    "# training state\n",
    "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)y_\n",
    "\n",
    "# transfer arrays in the state to specified devices and form ShardedDeviceArrays\n",
    "state = device_put_replicated(state, local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# perform training now\n",
    "# Apply the transform jax.pmap on the train_step to parallelize it on XLA devices\n",
    "# While vmap vectorizes a function by adding a batch dimension to every primitive operation \n",
    "# in the function, pmap replicates the function and executes each replica \n",
    "# on its own XLA device in parallel.\n",
    "parallelized_train_step = pmap(train_step, axis_name=\"batch\")\n",
    "state, inference_state, history = train_and_evaluate(state, parallelized_train_step, validation_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def get_renderings(rays):\n",
    "    model_fn = lambda x: inference_state.apply_fn(\n",
    "        {\"params\": inference_state.params}, x\n",
    "    )\n",
    "\n",
    "    ray_origins, ray_directions = rays\n",
    "    rgb, depth, acc, disparity, opacities = perform_volume_rendering(\n",
    "        model_fn, ray_origins, ray_directions\n",
    "    )\n",
    "    rgb = (255 * jnp.clip(rgb, 0, 1)).astype(jnp.uint8)\n",
    "    return rgb, depth, acc, disparity, opacities\n",
    "\n",
    "\n",
    "# Create a 360 degree video of the 3D scene\n",
    "def get_frames():\n",
    "    video_angle = jnp.linspace(0.0, 360.0, 120, endpoint=False)\n",
    "    camera_to_world_transform = map(lambda th: pose_spherical(th, -30.0, 4.0), video_angle)\n",
    "    rays = np.stack(list(map(\n",
    "        lambda x: generate_rays(\n",
    "            img_ht, img_wid, focal, x[:3, :4]\n",
    "        ), camera_to_world_transform\n",
    "    )))\n",
    "    rgb_frames, depth_frames, acc_maps, disparity_maps, opacities = lax.map(get_renderings, rays)\n",
    "    rgb_frames = np.asarray(rgb_frames)\n",
    "    depth_frames = np.asarray(depth_frames)\n",
    "    acc_maps = np.asarray(acc_maps * 255.)\n",
    "    disparity_maps = np.asarray(disparity_maps * 255.)\n",
    "    return rgb_frames, depth_frames, acc_maps, disparity_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render the scene\n",
    "rgb_frames, depth_frames, acc_maps, disparity_maps = get_frames()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73f72bc72549e98a84f6f70716c3cfd3763eb776afa04f85c632de90851415ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
